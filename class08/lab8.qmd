---
title: "class 08 - unsupervised learning miniproject"
author: "jack olmstead"
format: gfm
---

Let's import the data first!

```{r import and explore data structure}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
# head(wisc.df)
str(wisc.df)
```

Let's remove the first row - diagnosis - because this is the thing we will be trying to predict.

```{r}
wisc.data <- wisc.df[,-1]
diagnoses <- wisc.df[,1]
```

> Q1. How many observations are in this dataset?

```{r}
sprintf("There are %i rows in this dataset", nrow(wisc.data))
```

> Q2. How many of the observations have a malignant diagnosis?

```{r}
sprintf("There are %s malignant diagnoses", sum(diagnoses == "M"))
sprintf("There are %s benign diagnoses", sum(diagnoses == "B"))

table(diagnoses)
```

> Q3. How many variables/features in the data are suffixed with \_mean?

```{r}
sufx_mean <- grep("*_mean", colnames(wisc.data), value=T)
num_sufx_mean = length(sufx_mean)
sprintf("There are %i features that end with the suffix '_mean'", num_sufx_mean)
```

> Q.What features are '_mean' features?

```{r}
sufx_mean
```

Now, let's start exploring our data. Before doing dimensionality reduction, it is important to check if the data need to be scaled. Recall two common reasons for scaling data include:

-   The input variables use different units of measurement.
-   The input variables have significantly different variances.

```{r exploring data}
avgs <- colMeans(wisc.data)
SDs <- as.data.frame(round(apply(wisc.data, 2, sd), 2))
SDs$names <- rownames(SDs)
colnames(SDs) <- c("sd", "feature")

library(ggplot2)

ggplot(SDs) +
  aes(x=feature, y=sd) +
  geom_col() +
  coord_flip()
```
### PCA
```{r PCA}
wisc_pca <- prcomp(wisc.data, scale=T)
summary(wisc_pca)
```

Let's make a scree plot!
```{r}
pca.var <-  wisc_pca$sdev^2
# proportion of variance
pct.var.explained <- pca.var / sum(pca.var)

plot(pct.var.explained, typ="o")
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

44.3% of the variance is captured by PC1.

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

Only 3 PCs are needed to capture > 70% of the variance.

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

6 PCs are needed to capture  > 90% of the variance.


Let's make our main results figure from our PCA (a.k.a PC/ordination plot)!
```{r}
pcs <- as.data.frame(wisc_pca$x)
ggplot(pcs) +
  aes(x=PC1, y=PC2, color=diagnoses) +
  geom_point() 
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

There is a tantalizing separation in PC space between malignant and benign biopsies. This makes me think something like k-means clustering could be a viable way to diagnose new biopsies.

### Clustering
First, let's get a scaled version of our data.
```{r}
wisc.data.scaled <- scale(wisc.data)
```

```{r hierarchical clustering}
wisc.dist <- dist(wisc.data.scaled)
wisc.hclust <- hclust(wisc.dist)

wisc.hclust
plot(wisc.hclust)

groups <- cutree(wisc.hclust, h=19)
table(groups)
```

This is not a very inspiring dendrogram using the scaled data. There is no great separation of clusters.

Let's get a cross-tabulation of the hclust groups and the actual diagnoses to see how the clusters correlate to diagnoses
```{r}
table(groups, diagnoses)
```

What if we try hierarchical clustering in PC space? Let's only use the first 3 PCs, based on our scree plot.
```{r}
wisc.data.pca <- wisc_pca$x[,1:3]
wisc.dist.pca <- dist(wisc.data.pca)
wisc.hclust.pca <- hclust(wisc.dist.pca, method="ward.D2")

plot(wisc.hclust.pca)

pca.hclust.groups <- cutree(wisc.hclust.pca, 2)
table(pca.hclust.groups, diagnoses)
```

Let's calculate accuracy based off of these h-clusters in PCA space
```{r}
# accuracy is number of correct diagnoses divided by number of total diagnoses
pct.acc <- (179 + 333) / length(diagnoses) * 100
pct.acc
```


```{r k-means}
wisc.kms <- kmeans(as.matrix(pcs$PC1, pcs$PC2), 2)
wisc.kms$cluster <- factor(wisc.kms$cluster)

# re-level wisc.kms$cluster, because it comes up later when trying to color points
wisc.kms$cluster <- relevel(wisc.kms$cluster, 2)

p.diagnoses <- ggplot(pcs) +
  aes(x=PC1, y=PC2, color=diagnoses) +
  geom_point(size=2, alpha=0.6, stroke=1) +
  labs(title="Assigned by actual diagnoses") +
  scale_color_brewer(palette = "Accent")

p.kmeans <- ggplot(pcs) +
  aes(x=PC1, y=PC2, color=wisc.kms$cluster) +
  geom_point(size=2, alpha=0.6, stroke=1) +
  labs(title="Assigned by K-means clusters") +
    scale_color_brewer(palette = "Accent", direction=1)

library(patchwork)
p.diagnoses + p.kmeans + plot_layout(ncol=1)
```

Hmm... the k-means clustering looks okay, but the demarcation line between clusters relies almost exclusively on PC1, and almost not at all on PC2.















